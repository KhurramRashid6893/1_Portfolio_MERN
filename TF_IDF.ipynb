{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlazpiLg7pJLW9gJWRRCVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhurramRashid6893/1_Portfolio_MERN/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXU7h07hPTuh",
        "outputId": "169e2412-374c-426a-b255-6c3576930da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|words                                |features                                                                                     |\n",
            "+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|[i, love, ai, and, machine, learning]|(20,[0,11,12,13,16],[0.8109302162163288,0.0,0.0,0.0,0.4054651081081644])                     |\n",
            "|[deep, learning, and, ai, are, fun]  |(20,[2,3,11,12,13,19],[0.4054651081081644,0.4054651081081644,0.0,0.0,0.0,0.4054651081081644])|\n",
            "+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TFIDFExample\").getOrCreate()\n",
        "\n",
        "# Sample documents\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, \"I love AI and machine learning\"),\n",
        "    (1, \"Deep learning and AI are fun\")\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "# Compute Term Frequency\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "# Compute IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"words\", \"features\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# --- Step 1: Spark Session ---\n",
        "spark = SparkSession.builder.appName(\"TFIDFExample\").getOrCreate()\n",
        "\n",
        "# --- Step 2: Sample documents ---\n",
        "docs = [\n",
        "    (0, \"I love AI and machine learning\"),\n",
        "    (1, \"Deep learning and AI are fun\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(docs, [\"id\", \"sentence\"])\n",
        "\n",
        "# --- Step 3: Tokenize ---\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(df)\n",
        "\n",
        "# --- Step 4: Term Frequency ---\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "# --- Step 5: IDF ---\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "# --- Step 6: Explode words to print TF values ---\n",
        "# Compute raw TF counts\n",
        "wordsData_exploded = wordsData.select(\"id\", explode(\"words\").alias(\"word\"))\n",
        "\n",
        "# Count term frequency per document\n",
        "tf_counts = wordsData_exploded.groupBy(\"id\", \"word\").count()\n",
        "tf_counts.show()\n",
        "\n",
        "# --- Step 7: Show TF-IDF vectors ---\n",
        "rescaledData.select(\"id\", \"words\", \"tfidf_features\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJOlTQciPrxk",
        "outputId": "78b6cc2a-c3fc-4d83-f9c0-27a392aaae76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-----+\n",
            "| id|    word|count|\n",
            "+---+--------+-----+\n",
            "|  0|     and|    1|\n",
            "|  0| machine|    1|\n",
            "|  0|       i|    1|\n",
            "|  0|    love|    1|\n",
            "|  0|learning|    1|\n",
            "|  0|      ai|    1|\n",
            "|  1|     fun|    1|\n",
            "|  1|learning|    1|\n",
            "|  1|     are|    1|\n",
            "|  1|      ai|    1|\n",
            "|  1|     and|    1|\n",
            "|  1|    deep|    1|\n",
            "+---+--------+-----+\n",
            "\n",
            "+---+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|id |words                                |tfidf_features                                                                               |\n",
            "+---+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|0  |[i, love, ai, and, machine, learning]|(20,[0,11,12,13,16],[0.8109302162163288,0.0,0.0,0.0,0.4054651081081644])                     |\n",
            "|1  |[deep, learning, and, ai, are, fun]  |(20,[2,3,11,12,13,19],[0.4054651081081644,0.4054651081081644,0.0,0.0,0.0,0.4054651081081644])|\n",
            "+---+-------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKV3SfMQNYt"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.sql.functions import explode, col\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "# --- Step 1: Spark session ---\n",
        "spark = SparkSession.builder.appName(\"TFIDFWordImportance\").getOrCreate()\n",
        "\n",
        "# --- Step 2: Sample documents ---\n",
        "docs = [\n",
        "    (0, \"I love AI and machine learning\"),\n",
        "    (1, \"Deep learning and AI are fun\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(docs, [\"id\", \"sentence\"])\n",
        "\n",
        "# --- Step 3: Tokenize ---\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(df)\n",
        "\n",
        "# --- Step 4: Compute TF ---\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "# --- Step 5: Compute IDF ---\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "# --- Step 6: Explode words to get TF counts ---\n",
        "words_exploded = wordsData.select(\"id\", explode(\"words\").alias(\"word\"))\n",
        "tf_counts = words_exploded.groupBy(\"id\", \"word\").count()\n",
        "\n",
        "# --- Step 7: Join with TF-IDF ---\n",
        "# Convert TF-IDF sparse vector to dense array for each document and pair with words\n",
        "def tfidf_per_word(row):\n",
        "    words = row['words']\n",
        "    tfidf_vector = row['tfidf_features']\n",
        "    tfidf_values = tfidf_vector.toArray()\n",
        "    # Use HashingTF to get the index for each word\n",
        "    word_indices = [hashingTF.indexOf(w) for w in words]\n",
        "    # Pair each word with its TF-IDF value using the correct index\n",
        "    return [(row['id'], w, tfidf_values[idx]) for w, idx in zip(words, word_indices)]\n",
        "\n",
        "tfidf_rdd = rescaledData.rdd.flatMap(tfidf_per_word)\n",
        "tfidf_df = spark.createDataFrame(tfidf_rdd, [\"id\", \"word\", \"tfidf\"])\n",
        "\n",
        "# --- Step 8: Join TF and TF-IDF ---\n",
        "tf_tfidf_df = tf_counts.join(tfidf_df, on=['id','word']).orderBy('id','tfidf', ascending=False)\n",
        "tf_tfidf_df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT4xKrHeQKov"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.sql.functions import explode, col\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# --- Step 1: Spark session ---\n",
        "spark = SparkSession.builder.appName(\"TFIDFTopBottom\").getOrCreate()\n",
        "\n",
        "# --- Step 2: Sample documents ---\n",
        "docs = [\n",
        "    (0, \"I love AI and machine learning\"),\n",
        "    (1, \"Deep learning and AI are fun\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(docs, [\"id\", \"sentence\"])\n",
        "\n",
        "# --- Step 3: Tokenize ---\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(df)\n",
        "\n",
        "# --- Step 4: Compute TF ---\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "# --- Step 5: Compute IDF ---\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf_features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "# --- Step 6: Explode words to get TF counts ---\n",
        "words_exploded = wordsData.select(\"id\", explode(\"words\").alias(\"word\"))\n",
        "tf_counts = words_exploded.groupBy(\"id\", \"word\").count()\n",
        "\n",
        "# --- Step 7: Flatten TF-IDF vectors ---\n",
        "def tfidf_per_word(row):\n",
        "    words = row['words']\n",
        "    tfidf_vector = row['tfidf_features']\n",
        "    tfidf_values = tfidf_vector.toArray()\n",
        "    # Use HashingTF to get the index for each word\n",
        "    word_indices = [hashingTF.indexOf(w) for w in words]\n",
        "    # Pair each word with its TF-IDF value using the correct index\n",
        "    return [(row['id'], w, tfidf_values[idx]) for w, idx in zip(words, word_indices)]\n",
        "\n",
        "tfidf_rdd = rescaledData.rdd.flatMap(tfidf_per_word)\n",
        "tfidf_df = spark.createDataFrame(tfidf_rdd, [\"id\", \"word\", \"tfidf\"])\n",
        "\n",
        "# --- Step 8: Join TF counts with TF-IDF ---\n",
        "tf_tfidf_df = tf_counts.join(tfidf_df, on=['id','word'])\n",
        "\n",
        "# --- Step 9: Find highest and lowest TF-IDF words per document ---\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window = Window.partitionBy(\"id\")\n",
        "\n",
        "# Highest TF-IDF\n",
        "high_df = tf_tfidf_df.withColumn(\"rank_high\", F.row_number().over(window.orderBy(F.desc(\"tfidf\")))) \\\n",
        "                     .filter(col(\"rank_high\") == 1) \\\n",
        "                     .select(\"id\", col(\"word\").alias(\"high_word\"), \"count\", col(\"tfidf\").alias(\"high_tfidf\"))\n",
        "\n",
        "# Lowest TF-IDF\n",
        "low_df = tf_tfidf_df.withColumn(\"rank_low\", F.row_number().over(window.orderBy(\"tfidf\"))) \\\n",
        "                    .filter(col(\"rank_low\") == 1) \\\n",
        "                    .select(\"id\", col(\"word\").alias(\"low_word\"), \"count\", col(\"tfidf\").alias(\"low_tfidf\"))\n",
        "\n",
        "# Join high and low\n",
        "result = high_df.join(low_df, on=\"id\")\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}